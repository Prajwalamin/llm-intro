{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8fde4c-9222-4265-b72b-8d7693520250",
   "metadata": {},
   "source": [
    "#  Introduction to Large Language Models with GPT & LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e302e1c-4c18-4c44-87fd-ba935c3a0853",
   "metadata": {},
   "source": [
    "[ChatGPT](https://chat.openai.com/) is wildly popular, with over a billion visits per month. Although this web interface is great for many non-technical use cases, for programming and automation tasks, it is better to access GPT (the AI that powers ChatGPT) via the OpenAI API.\n",
    "\n",
    "As well as GPT, you'll also make use of LangChain, a programming framework for working with generative AI.\n",
    "\n",
    "You'll cover:\n",
    "\n",
    "- Getting set up with an OpenAI developer account and integration with Workspace.\n",
    "- Calling the chat functionality in the OpenAI API, with and without langchain.\n",
    "- Simple prompt engineering.\n",
    "- Holding a conversation with GPT.\n",
    "- Ideas for incorporating GPT into a data analysis or data science workflow.\n",
    "\n",
    "You'll be using GPT to explore [a dataset](https://catalog.data.gov/dataset/electric-vehicle-population-data) about electric cars in Washington state, USA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231d2c6-275e-4399-b7cd-84e112831d08",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d7fac-edb2-482f-be2b-c63dc2882103",
   "metadata": {},
   "source": [
    "You'll need a developer account with OpenAI.\n",
    "\n",
    "See *getting-started.ipynb* for steps on how to create an API key and store it in Workspace. In particular, you'll need to follow the instructions in the \"Getting started with OpenAI\" and \"Setting up Workspace Integrations\" sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9274661-8d8c-4cc5-901e-5fc497866b89",
   "metadata": {},
   "source": [
    "## Task 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf847fd-f8f8-49f6-9b43-0eb098239072",
   "metadata": {},
   "source": [
    "We need to install the `langchain` package. This is currently being developed quickly, sometimes with breaking changes, so we fix the version.\n",
    "\n",
    "The `langchain` depends on a recent version of `typing_extensions`, so we need to update that package, again fixing the version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfbc35-bc10-415d-818d-18697c7e1988",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the following code to install `langchain` and `typing_extensions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67f9639-2c6e-43e2-8681-46c5a42c1b12",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 25281,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1714642030364,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Lock OpenAI to version 0.27.1\n!pip install openai==0.27.1\n# Install the langchain package\n!pip install langchain==0.0.300",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.27.1\n",
      "  Downloading openai-0.27.1.tar.gz (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from openai==0.27.1) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from openai==0.27.1) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from openai==0.27.1) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai==0.27.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai==0.27.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai==0.27.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai==0.27.1) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai==0.27.1) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai==0.27.1) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai==0.27.1) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai==0.27.1) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai==0.27.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai==0.27.1) (1.2.0)\n",
      "Building wheels for collected packages: openai\n",
      "  Building wheel for openai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai: filename=openai-0.27.1-py3-none-any.whl size=70084 sha256=8edbbcade24ec998748468f98a21ed8768306c89ed53a3ee20e09304d2f87187\n",
      "  Stored in directory: /Users/prajwalamin/Library/Caches/pip/wheels/1f/d1/75/8015df8f7ec8ba5422d8a45786cbb64d421872f488c09303fe\n",
      "Successfully built openai\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.24.1\n",
      "    Uninstalling openai-1.24.1:\n",
      "      Successfully uninstalled openai-1.24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.1.4 requires openai<2.0.0,>=1.10.0, but you have openai 0.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.27.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting langchain==0.0.300\n",
      "  Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (3.8.3)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (3.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (4.0.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.300)\n",
      "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (1.33)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.300)\n",
      "  Downloading numexpr-2.10.0-cp39-cp39-macosx_10_9_x86_64.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (1.21.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from langchain==0.0.300) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.300) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from anyio<4.0->langchain==0.0.300) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.300) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.300) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1->langchain==0.0.300) (4.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.300) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.300) (2022.12.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (1.1.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/prajwalamin/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.4.3)\n",
      "Installing collected packages: typing-inspect, numexpr, marshmallow, dataclasses-json, langsmith, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.3\n",
      "    Uninstalling numexpr-2.8.3:\n",
      "      Successfully uninstalled numexpr-2.8.3\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.52\n",
      "    Uninstalling langsmith-0.1.52:\n",
      "      Successfully uninstalled langsmith-0.1.52\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-core 0.1.48 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n",
      "langchain-openai 0.1.4 requires openai<2.0.0,>=1.10.0, but you have openai 0.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dataclasses-json-0.6.5 langchain-0.0.300 langsmith-0.0.92 marshmallow-3.21.2 numexpr-2.10.0 typing-inspect-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Lock OpenAI to version 0.27.1\n",
    "!pip install openai==0.27.1\n",
    "# Install the langchain package\n",
    "!pip install langchain==0.0.300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33bdeab-3888-4032-995d-9f4589133cbf",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5462,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1714642131703,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Update the typing_extensions package\n!pip install typing_extensions==4.8.0",
    "outputsMetadata": {
     "0": {
      "height": 175,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typing_extensions==4.8.0\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: typing_extensions\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyzmq>=22.1.0, but you have pyzmq 19.0.2 which is incompatible.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing_extensions-4.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update the typing_extensions package\n",
    "!pip install typing_extensions==4.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787756e-02d2-483b-8da2-482366103c8f",
   "metadata": {},
   "source": [
    "In order to chat with GPT, we need first need to load the `openai` and `os` packages to set the API key from the environment variables you just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410f09e-ffc1-4e82-8f7d-0bab1ed08e43",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import the `os` package.\n",
    "- Import the `openai` package.\n",
    "- Set `openai.api_key` to the `OPENAI_API_KEY` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce3946a-d0a7-4a89-96ae-6ad8ac8ca2c8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3362,
    "lastExecutedAt": 1714642138292,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the os package\nimport os\n\n# Import the openai package\nimport openai\n\n# Set openai.api_key to the OPENAI_API_KEY environment variable\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   },
   "outputs": [],
   "source": [
    "# Import the os package\n",
    "import os\n",
    "\n",
    "# Import the openai package\n",
    "import openai\n",
    "\n",
    "# Set openai.api_key to the OPENAI_API_KEY environment variable\n",
    "openai.api_key = [\"INSERT YOUR OWN API KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdeb78-f229-4d3c-8cac-9b2d6c06351e",
   "metadata": {},
   "source": [
    "We need to import the `langchain` package. It has many submodules, so to save typing later, we'll also import some specific functions from those submodules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a21ba-c6ce-4474-b386-e27add3bd35c",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Import the `langchain` package as `lc`.\n",
    "- From the `langchain.chat_models` module, import `ChatOpenAI`.\n",
    "- From the `langchain.schema` module, import `AIMessage`, `HumanMessage`, `SystemMessage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3971bc6-1bbd-4e00-9b75-7ad51cfa14e3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1714645742885,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import the langchain package as lc\nimport langchain as lc\n\n# From the langchain.chat_models module, import ChatOpenAI\nfrom langchain.chat_models import ChatOpenAI\n\n# From the langchain.schema module, import AIMessage, HumanMessage, SystemMessage\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the langchain package as lc\n",
    "import langchain as lc\n",
    "\n",
    "# From the langchain.chat_models module, import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# From the langchain.schema module, import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98530c9-aca9-45bc-8da3-072b78b0c08d",
   "metadata": {},
   "source": [
    "You'll also need to do some light data manipulation with the `pandas` package and data visualization with `plotly.express`.  Finally, the `IPython.display` pacakges contains functions to prettily display Markdown content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9caca-70fd-4ac0-aa15-1bee55c456d3",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcd794-b29c-4010-8be0-651a452b2044",
   "metadata": {},
   "source": [
    "Import the following packages.\n",
    "\n",
    "- Import `pandas` using the alias `pd`.\n",
    "- Import `plotly.express` using the alias `px`.\n",
    "- From the `IPython.display` package, import `display` and `Markdown`.\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81d6d0e-986b-49f4-94e1-7315a7f0bd67",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 298,
    "lastExecutedAt": 1714642149759,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import pandas using the alias pd\nimport pandas as pd\n\n# Import plotly.express using the alias px\nimport plotly.express as px\n\n# From the IPython.display package, import display and Markdown\nfrom IPython.display import display, Markdown "
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fc-list: major issue: So far, no MiKTeX administrator has checked for updates.\n",
      "fc-list: major issue: So far, no MiKTeX administrator has checked for updates.\n"
     ]
    }
   ],
   "source": [
    "# Import pandas using the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Import plotly.express using the alias px\n",
    "import plotly.express as px\n",
    "\n",
    "# From the IPython.display package, import display and Markdown\n",
    "from IPython.display import display, Markdown "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c51c7-bbbb-4f0f-b218-850221f3dcdf",
   "metadata": {},
   "source": [
    "## Task 1: Import the Electric Cars Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce00845-9741-4979-91aa-e43ff0db5299",
   "metadata": {},
   "source": [
    "The electric cars data is contained in a CSV file named `electric_cars.csv`.\n",
    "\n",
    "Each row in the dataset represents the count of the number of cars registered within a city, for a particular model.\n",
    "\n",
    "The dataset contains the following columns.\n",
    "\n",
    "- `city` (character): The city in which the registered owner resides.\n",
    "- `county` (character): The county in which the registered owner resides.\n",
    "- `model_year` (integer): The [model year](https://en.wikipedia.org/wiki/Model_year#United_States_and_Canada) of the car.\n",
    "- `make` (character): The manufacturer of the car.\n",
    "- `model` (character): The model of the car.\n",
    "- `electric_vehicle_type` (character): Either \"Plug-in Hybrid Electric Vehicle (PHEV)\" or \"Battery Electric Vehicle (BEV)\".\n",
    "- `n_cars` (integer): The count of the number of vehicles registered.\n",
    "\n",
    "Our first step is to import and print the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b998f52-93c9-411c-ba74-473955de4b8e",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19bfda1-fe9a-4344-9dd9-613d33598d21",
   "metadata": {},
   "source": [
    "Import the electric cars data to a pandas dataframe.\n",
    "\n",
    "- Read the data from `electric_cars.csv`. Assign to `electric_cars`.\n",
    "- Display a description of the numeric columns of `electric_cars`.\n",
    "- Display a description of the object columns of `electric_cars`.\n",
    "- Print the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474d98a8-33ce-4898-a641-4853f17e5738",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 258,
    "lastExecutedAt": 1714642152113,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Read the data from electric_cars.csv. Assign to electric_cars.\nelectric_cars = pd.read_csv('electric_cars.csv')\n\n# Display a description of the numeric columns\nprint(\"Description of numeric columns\\n\")\ndisplay(electric_cars.describe())\n\n# Display a description of the text (object) columns\nprint(\"Description of text columns\\n\")\ndisplay(electric_cars.describe(include=\"O\"))\n\n# Print the whole dataset\nprint(\"The electric cars dataset\\n\")\ndisplay(electric_cars)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     },
     "1": {
      "height": 271,
      "type": "dataFrame"
     },
     "2": {
      "height": 59,
      "type": "stream"
     },
     "3": {
      "height": 171,
      "type": "dataFrame"
     },
     "4": {
      "height": 59,
      "type": "stream"
     },
     "5": {
      "height": 321,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of numeric columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_year</th>\n",
       "      <th>n_cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26813.000000</td>\n",
       "      <td>26813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019.375527</td>\n",
       "      <td>5.612166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.286257</td>\n",
       "      <td>26.997325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1997.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024.000000</td>\n",
       "      <td>1514.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model_year        n_cars\n",
       "count  26813.000000  26813.000000\n",
       "mean    2019.375527      5.612166\n",
       "std        3.286257     26.997325\n",
       "min     1997.000000      1.000000\n",
       "25%     2017.000000      1.000000\n",
       "50%     2020.000000      2.000000\n",
       "75%     2022.000000      4.000000\n",
       "max     2024.000000   1514.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of text columns\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>electric_vehicle_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "      <td>26813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>683</td>\n",
       "      <td>183</td>\n",
       "      <td>37</td>\n",
       "      <td>127</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Bothell</td>\n",
       "      <td>King</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>LEAF</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>479</td>\n",
       "      <td>7066</td>\n",
       "      <td>5071</td>\n",
       "      <td>1889</td>\n",
       "      <td>15885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           city county   make  model           electric_vehicle_type\n",
       "count     26813  26813  26813  26813                           26813\n",
       "unique      683    183     37    127                               2\n",
       "top     Bothell   King  TESLA   LEAF  Battery Electric Vehicle (BEV)\n",
       "freq        479   7066   5071   1889                           15885"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The electric cars dataset\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>model_year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>electric_vehicle_type</th>\n",
       "      <th>n_cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>King</td>\n",
       "      <td>2023</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL Y</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>King</td>\n",
       "      <td>2018</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL 3</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>King</td>\n",
       "      <td>2021</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL Y</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>King</td>\n",
       "      <td>2022</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL Y</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bellevue</td>\n",
       "      <td>King</td>\n",
       "      <td>2023</td>\n",
       "      <td>TESLA</td>\n",
       "      <td>MODEL Y</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26808</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>Pierce</td>\n",
       "      <td>2022</td>\n",
       "      <td>BMW</td>\n",
       "      <td>IX</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26809</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>Pierce</td>\n",
       "      <td>2022</td>\n",
       "      <td>BMW</td>\n",
       "      <td>X5</td>\n",
       "      <td>Plug-in Hybrid Electric Vehicle (PHEV)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26810</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>Pierce</td>\n",
       "      <td>2022</td>\n",
       "      <td>FORD</td>\n",
       "      <td>TRANSIT</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26811</th>\n",
       "      <td>Lakewood</td>\n",
       "      <td>Pierce</td>\n",
       "      <td>2022</td>\n",
       "      <td>HYUNDAI</td>\n",
       "      <td>KONA ELECTRIC</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26812</th>\n",
       "      <td>Zillah</td>\n",
       "      <td>Yakima</td>\n",
       "      <td>2023</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>ID.4</td>\n",
       "      <td>Battery Electric Vehicle (BEV)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26813 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           city  county  model_year        make          model  \\\n",
       "0       Seattle    King        2023       TESLA        MODEL Y   \n",
       "1       Seattle    King        2018       TESLA        MODEL 3   \n",
       "2       Seattle    King        2021       TESLA        MODEL Y   \n",
       "3       Seattle    King        2022       TESLA        MODEL Y   \n",
       "4      Bellevue    King        2023       TESLA        MODEL Y   \n",
       "...         ...     ...         ...         ...            ...   \n",
       "26808  Lakewood  Pierce        2022         BMW             IX   \n",
       "26809  Lakewood  Pierce        2022         BMW             X5   \n",
       "26810  Lakewood  Pierce        2022        FORD        TRANSIT   \n",
       "26811  Lakewood  Pierce        2022     HYUNDAI  KONA ELECTRIC   \n",
       "26812    Zillah  Yakima        2023  VOLKSWAGEN           ID.4   \n",
       "\n",
       "                        electric_vehicle_type  n_cars  \n",
       "0              Battery Electric Vehicle (BEV)    1514  \n",
       "1              Battery Electric Vehicle (BEV)    1153  \n",
       "2              Battery Electric Vehicle (BEV)    1147  \n",
       "3              Battery Electric Vehicle (BEV)    1122  \n",
       "4              Battery Electric Vehicle (BEV)     931  \n",
       "...                                       ...     ...  \n",
       "26808          Battery Electric Vehicle (BEV)       1  \n",
       "26809  Plug-in Hybrid Electric Vehicle (PHEV)       1  \n",
       "26810          Battery Electric Vehicle (BEV)       1  \n",
       "26811          Battery Electric Vehicle (BEV)       1  \n",
       "26812          Battery Electric Vehicle (BEV)       1  \n",
       "\n",
       "[26813 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read the data from electric_cars.csv. Assign to electric_cars.\n",
    "electric_cars = pd.read_csv('electric_cars.csv')\n",
    "\n",
    "# Display a description of the numeric columns\n",
    "print(\"Description of numeric columns\\n\")\n",
    "display(electric_cars.describe())\n",
    "\n",
    "# Display a description of the text (object) columns\n",
    "print(\"Description of text columns\\n\")\n",
    "display(electric_cars.describe(include=\"O\"))\n",
    "\n",
    "# Print the whole dataset\n",
    "print(\"The electric cars dataset\\n\")\n",
    "display(electric_cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65454beb-970f-4af6-a04c-798b9f665b6f",
   "metadata": {},
   "source": [
    "## Task 2: Asking GPT a Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb43b1-9455-456e-a674-34b58e2faae2",
   "metadata": {},
   "source": [
    "Let's start by sending a message to GPT and getting a response. For now, we won't worry about including any details about the dataset&mdash;it's the equivalent of asking \"is this microphone turned on?\".\n",
    "\n",
    "We'll also skip using langchain for now so you can see more clearly how the `openai` packages works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4d331-7494-49c5-9d46-338e7b62b30a",
   "metadata": {},
   "source": [
    "### Types of Message\n",
    "\n",
    "There are three types of message, documented in the [Introduction](https://platform.openai.com/docs/guides/chat/introduction) to the Chat documentation. We'll use two of them here.\n",
    "\n",
    "- `system` messages describe the behavior of the AI assistant. If you don't know what you want, try \"You are a helpful assistant\".\n",
    "- `user` messages describe what you want the AI assistant to say. We'll cover examples of this today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134cdbf-1fc6-46fc-8075-6211db727739",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a3334-8917-41cb-9300-3977744e702b",
   "metadata": {},
   "source": [
    "Send a question to GPT and get a response.\n",
    "\n",
    "- Define the system message as follows and assign to `system_msg_test`.\n",
    "\n",
    "```\n",
    "\"\"\"You are a helpful assistant who understands data science.\n",
    " You write in a clear language that a ten year old can understand.\n",
    " You keep your answers brief.\"\"\". \n",
    "```\n",
    "    \n",
    "- Define the user message as follows and assign to `user_msg_test`.\n",
    "\n",
    "```\n",
    "\"Tell me some uses of GPT for data analysis.\"\n",
    "```\n",
    "\n",
    "- Create a message list from the system and user messages. Assign to `msgs_test`.\n",
    "- Send the messages to GPT. Assign to `rsps_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6638162-655b-41b6-8436-ae57fab93fab",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "        \n",
    "The `openai.ChatCompletion.create()` function expects the messages in the form of a list of dictionaries, each with a `role` and `content` element.\n",
    "        \n",
    "```\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": user_msg}\n",
    "]\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1314,
    "id": "bA5ajAmk7XH6",
    "lastExecutedAt": 1714642172150,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Define the system message. Assign to system_msg_test.\nsystem_msg_test = \"\"\"You are a helpful assistant who understands data science.\n You write in a clear language that a ten year old can understand.\n You keep your answers brief.\"\"\"\n\n# Define the user message. Assign to user_msg_test.\nuser_msg_test = \"Tell me about trees\"\n\n# Create a message list from the system and user messages. Assign to msgs_test.\nmsg_test = [\n    {'role': \"system\", 'content': system_msg_test},\n    {'role': 'user', 'content': user_msg_test}\n]\n\n# Send the messages to GPT. Assign to rsps_test.\nrsps_test = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=msg_test\n)"
   },
   "outputs": [],
   "source": [
    "# Define the system message. Assign to system_msg_test.\n",
    "system_msg_test = \"\"\"You are a helpful assistant who understands data science.\n",
    " You write in a clear language that a ten year old can understand.\n",
    " You keep your answers brief.\"\"\"\n",
    "\n",
    "# Define the user message. Assign to user_msg_test.\n",
    "user_msg_test = \"Give me some information about nissan cars\"\n",
    "\n",
    "# Create a message list from the system and user messages. Assign to msgs_test.\n",
    "msg_test = [\n",
    "    {'role': \"system\", 'content': system_msg_test},\n",
    "    {'role': 'user', 'content': user_msg_test}\n",
    "]\n",
    "\n",
    "# Send the messages to GPT. Assign to rsps_test.\n",
    "rsps_test = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=msg_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532c586-fa2e-45e5-9d07-2188b8f11f46",
   "metadata": {},
   "source": [
    "Now you need to explore the response. The result is a highly nested object. As well as the text response that we want, there's a lot of metadata. You'll print the whole thing so you can see the structure, and extract just the text content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc4dab-01d7-4027-8e90-da027cd9bfd9",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Print the whole response and just the text content.\n",
    "\n",
    "- Print the whole response.\n",
    "- Print just the response's content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab282fd8-04dc-48b8-81ac-a5ffed9176b0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Code hints</summary>\n",
    "<p>\n",
    "        \n",
    "Buried within the response variable is the text we asked GPT to generate. Luckily, it's always in the same place.\n",
    "        \n",
    "```\n",
    "response[\"choices\"][0][\"message\"][\"content\"]\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20279022-1034-4fc8-8c34-deb411dfeebd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1714642174624,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Print the whole response\nprint(\"The whole response\\n\")\nprint(rsps_test)\n\nprint(\"\\n\\n----\\n\\n\")\n\n\n# Print just the response's content\nprint(\"Just the response's content\\n\")\nprint(rsps_test[\"choices\"][0][\"message\"][\"content\"])",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole response\n",
      "\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Nissan makes cars that people can drive. They come in different shapes and sizes like sedans, SUVs, and trucks. People like Nissan cars because they are reliable and have cool features.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1714647390,\n",
      "  \"id\": \"chatcmpl-9KOP0qgVpRfFkXcj0sKzFHLAFDgWH\",\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_a450710239\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 39,\n",
      "    \"prompt_tokens\": 49,\n",
      "    \"total_tokens\": 88\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "\n",
      "Just the response's content\n",
      "\n",
      "Nissan makes cars that people can drive. They come in different shapes and sizes like sedans, SUVs, and trucks. People like Nissan cars because they are reliable and have cool features.\n"
     ]
    }
   ],
   "source": [
    "# Print the whole response\n",
    "print(\"The whole response\\n\")\n",
    "print(rsps_test)\n",
    "\n",
    "print(\"\\n\\n----\\n\\n\")\n",
    "\n",
    "\n",
    "# Print just the response's content\n",
    "print(\"Just the response's content\\n\")\n",
    "print(rsps_test[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f8b8b-5ef6-412a-a432-74dd103c141c",
   "metadata": {},
   "source": [
    "## Task 3: Asking a Question About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58127a5-39d4-491e-8f31-7e90f1da6e8e",
   "metadata": {},
   "source": [
    "Now we know that GPT is working, we can start asking questions about data analysis. Because we have details of our dataset, we can pass these in to our prompt to improve the quality of the mesages we get back.\n",
    "\n",
    "Another change that we're going to make is to use the `langchain` package, which provides a convenience layer on top of the `openai` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a0009-54b6-42f6-a98a-317168b85053",
   "metadata": {},
   "source": [
    "### Why should we use LangChain?\n",
    "\n",
    "The code in the previous task used complicated nested objects in two places (the list of dictionaries for the message, and the dictionary of lists and dictionaries for the response). This sort of object is common in web application development, but not in data analysis, where rectangular data (pandas DataFrames and SQL tables) is the more common.\n",
    "\n",
    "One of the advantages of LangChain is that it simplifies the code for some tasks, letting you avoid messing about with too many square brackets and curly braces as you navigate these deep objects.\n",
    "\n",
    "Secondly, if you want to swap GPT for a different model at a later date (as you might in a corporate setting), it can be easier to do so if you use the `langchain` package rather than the `openai` package directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38d1fa-e17f-48c6-b7c8-428c491fcd44",
   "metadata": {},
   "source": [
    "### LangChain message types\n",
    "\n",
    "The LangChain message types are names slightly differently from the OpenAI message types.\n",
    "\n",
    "- `SystemMessage` is the equivalent of OpenAI's `system` message.\n",
    "- `HumanMessage` is the equivalent of OpenAI's `user` message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852a0d9-5a94-4611-89af-30c401b3d398",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Create a prompt that includes dataset details.\n",
    "\n",
    "- _Read the description of the dataset that is provided._\n",
    "- Create a task for the AI. Assign to `suggest_questions`.\n",
    "    - Use the text `\"Suggest some data analysis questions that could be answered with this dataset.\"`.\n",
    "- Concatenate the dataset description and the request. Assign to `msgs_suggest_questions`.\n",
    "    - The first message is a system message with the content `\"You are a data analysis expert.\"`.\n",
    "    - The second message is a human message with `dataset_description` and `suggest_questions` concatenated with two line breaks in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "858797d3-ca37-4952-9a10-1f17b6703df0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "lastExecutedAt": 1714642518750,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# A description of the dataset\ndataset_description = \"\"\"\nYou have a dataset about electric cars registered in Washington state, USA in 2020. It is available as a pandas DataFrame named `electric_cars`.\n\nEach row in the dataset represents the count of the number of cars registered within a city, for a particular model.\n\nThe dataset contains the following columns.\n\n- `city` (character): The city in which the registered owner resides.\n- `county` (character): The county in which the registered owner resides.\n- `model_year` (integer): The [model year](https://en.wikipedia.org/wiki/Model_year#United_States_and_Canada) of the car.\n- `make` (character): The manufacturer of the car.\n- `model` (character): The model of the car.\n- `electric_vehicle_type` (character): Either \"Plug-in Hybrid Electric Vehicle (PHEV)\" or \"Battery Electric Vehicle (BEV)\".\n- `n_cars` (integer): The count of the number of vehicles registered.\n\"\"\"\n\n# Create a task for the AI. Assign to suggest_questions.\nsuggested_questions = \"Suggest some data analysis questions that could be answered with this dataset.\"\n\n# Concatenate the dataset description and the request. Assign to msgs_suggest_questions.\nmsgs_suggest_questions = [\n    SystemMessage(content=\"You are a data analysis expert.\"),\n    HumanMessage(content=f\"{dataset_description}\\n\\n{suggested_questions}\")\n]\n"
   },
   "outputs": [],
   "source": [
    "# A description of the dataset\n",
    "dataset_description = \"\"\"\n",
    "You have a dataset about electric cars registered in Washington state, USA in 2020. It is available as a pandas DataFrame named `electric_cars`.\n",
    "\n",
    "Each row in the dataset represents the count of the number of cars registered within a city, for a particular model.\n",
    "\n",
    "The dataset contains the following columns.\n",
    "\n",
    "- `city` (character): The city in which the registered owner resides.\n",
    "- `county` (character): The county in which the registered owner resides.\n",
    "- `model_year` (integer): The [model year](https://en.wikipedia.org/wiki/Model_year#United_States_and_Canada) of the car.\n",
    "- `make` (character): The manufacturer of the car.\n",
    "- `model` (character): The model of the car.\n",
    "- `electric_vehicle_type` (character): Either \"Plug-in Hybrid Electric Vehicle (PHEV)\" or \"Battery Electric Vehicle (BEV)\".\n",
    "- `n_cars` (integer): The count of the number of vehicles registered.\n",
    "\"\"\"\n",
    "\n",
    "# Create a task for the AI. Assign to suggest_questions.\n",
    "suggested_questions = \"Suggest some data analysis questions that could be answered with this dataset.\"\n",
    "\n",
    "# Concatenate the dataset description and the request. Assign to msgs_suggest_questions.\n",
    "msgs_suggest_questions = [\n",
    "    SystemMessage(content=\"You are a data analysis expert.\"),\n",
    "    HumanMessage(content=f\"{dataset_description}\\n\\n{suggested_questions}\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f085a-abc2-48c1-b77a-f655a1441d8e",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Create a `ChatOpenAI` object. Assign to `chat`.\n",
    "- Pass your message to GPT. Assign to `rsps_suggest_questions`.\n",
    "- Print the response object and the contents of the response.\n",
    "- Print the type of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93bb457c-134f-4349-81ae-c8c6c6bc1d30",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole response\n",
      "\n",
      "content='Here are some data analysis questions that could be explored using the provided dataset `electric_cars`:\\n\\n1. What is the distribution of electric car registrations by city in Washington state in 2020?\\n2. Which county in Washington state had the highest number of electric car registrations in 2020?\\n3. What is the most popular electric car model registered in Washington state in 2020?\\n4. How does the distribution of Plug-in Hybrid Electric Vehicles (PHEVs) compare to Battery Electric Vehicles (BEVs) in the dataset?\\n5. Is there a correlation between the model year of the car and the number of registrations?\\n6. Which manufacturer had the highest number of electric car registrations in Washington state in 2020?\\n7. Are there any cities or counties with a disproportionately high number of electric car registrations compared to their population size?\\n8. What is the average number of electric cars registered per city in Washington state in 2020?\\n9. How has the trend in electric car registrations changed over the years in Washington state?\\n10. Can we identify any seasonal patterns in electric car registrations based on the data for 2020?\\n\\nThese questions can provide valuable insights into the adoption and distribution of electric vehicles in Washington state, as well as help identify trends and patterns within the dataset.' additional_kwargs={} example=False\n",
      "\n",
      "----\n",
      "\n",
      "Just the response's content\n",
      "\n",
      "Here are some data analysis questions that could be explored using the provided dataset `electric_cars`:\n",
      "\n",
      "1. What is the distribution of electric car registrations by city in Washington state in 2020?\n",
      "2. Which county in Washington state had the highest number of electric car registrations in 2020?\n",
      "3. What is the most popular electric car model registered in Washington state in 2020?\n",
      "4. How does the distribution of Plug-in Hybrid Electric Vehicles (PHEVs) compare to Battery Electric Vehicles (BEVs) in the dataset?\n",
      "5. Is there a correlation between the model year of the car and the number of registrations?\n",
      "6. Which manufacturer had the highest number of electric car registrations in Washington state in 2020?\n",
      "7. Are there any cities or counties with a disproportionately high number of electric car registrations compared to their population size?\n",
      "8. What is the average number of electric cars registered per city in Washington state in 2020?\n",
      "9. How has the trend in electric car registrations changed over the years in Washington state?\n",
      "10. Can we identify any seasonal patterns in electric car registrations based on the data for 2020?\n",
      "\n",
      "These questions can provide valuable insights into the adoption and distribution of electric vehicles in Washington state, as well as help identify trends and patterns within the dataset.\n",
      "\n",
      "----\n",
      "\n",
      "The type of the response\n",
      "\n",
      "<class 'langchain.schema.messages.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "# Create a ChatOpenAI object. Assign to chat.\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# Pass your message to GPT. Assign to rsps_suggest_questions.\n",
    "rsps_suggest_questions = chat(msgs_suggest_questions)\n",
    "\n",
    "# Print the response\n",
    "print(\"The whole response\\n\")\n",
    "print(rsps_suggest_questions)\n",
    "\n",
    "print(\"\\n----\\n\")\n",
    "\n",
    "# Print just the response's content\n",
    "print(\"Just the response's content\\n\")\n",
    "print(rsps_suggest_questions.content)\n",
    "\n",
    "print(\"\\n----\\n\")\n",
    "\n",
    "# Print the type of the response\n",
    "print(\"The type of the response\\n\")\n",
    "print(type(rsps_suggest_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606312e-389f-4762-a324-f64c019247cf",
   "metadata": {},
   "source": [
    "## Task 4: Hold a conversation with GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8a719-267a-41bb-9c4a-da6fd3d670b2",
   "metadata": {},
   "source": [
    "Notice that the response from GPT was a dictionary-like object. The most useful part of this is the `.content` element, which contains the text response to your prompt.\n",
    "\n",
    "While a single prompt and response can be useful, often you want to have a longer conversation with GPT. In this case, you can pass previous messages so that GPT can \"remember\" what was said before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dde704-9fe1-4b5c-9ad3-5c15dc3fac2e",
   "metadata": {},
   "source": [
    "### AI messages\n",
    "\n",
    "The response from GPT had type `AIMessage`. By distinguishing `AIMessage`s from the `HumanMessage`s, you can tell who said what in a conversation with the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f45997-69d0-4e6b-9766-af82d482c28e",
   "metadata": {},
   "source": [
    "### Displaying Markdown content\n",
    "\n",
    "When GPT generates code as an output, it if often formatted as a Markdown code block inside triple backticks. You can display Markdown output more beautifully in Workspace by swapping `print()` for the `display()` and `Markdown()` functions.\n",
    "\n",
    "```py\n",
    "display(Markdown(your_markdown_text))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db3752-ff50-420e-89c9-0a2b88dbe848",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Append another prompt to the conversation and chat with GPT again.\n",
    "\n",
    "- Append the response and a new message to the previous messages. Assign to `msgs_python_top_models`.\n",
    "- Pass your message to GPT. Assign to `rsps_python_top_models`.\n",
    "- Display the response's Markdown content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9d287f-9c19-4c7b-b534-2de5a4ac3b18",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 7770,
    "lastExecutedAt": 1714644489565,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Append the response and a new message to the previous messages. \n# Assign to msgs_python_top_models.\nmsgs_python_top_models = msgs_suggest_questions + [\n    rsps_suggest_questions,\n    HumanMessage(content=\"Write Python code to find the make/model combinations of electric car in Washington state.\")\n]\n\n# Pass your message to GPT. Assign to rsps_python_top_models.\nrsps_python_top_models = chat(msgs_python_top_models)\n\n# Display the response's Markdown content\ndisplay(Markdown(rsps_python_top_models.content))"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find the make/model combinations of electric cars in Washington state from the provided `electric_cars` DataFrame, you can use the following Python code:\n",
       "\n",
       "```python\n",
       "# Filter the DataFrame to include only electric vehicles\n",
       "electric_vehicles = electric_cars[electric_cars['electric_vehicle_type'].str.contains('Electric Vehicle')]\n",
       "\n",
       "# Extract the unique make/model combinations\n",
       "make_model_combinations = electric_vehicles[['make', 'model']].drop_duplicates()\n",
       "\n",
       "# Print the make/model combinations\n",
       "print(make_model_combinations)\n",
       "```\n",
       "\n",
       "This code first filters the `electric_cars` DataFrame to include only electric vehicles by checking if the 'electric_vehicle_type' column contains the string 'Electric Vehicle'. Then, it extracts the unique combinations of 'make' and 'model' from the filtered DataFrame and prints the results.\n",
       "\n",
       "You can run this code snippet in your Python environment to find the make/model combinations of electric cars in Washington state."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Append the response and a new message to the previous messages. \n",
    "# Assign to msgs_python_top_models.\n",
    "msgs_python_top_models = msgs_suggest_questions + [\n",
    "    rsps_suggest_questions,\n",
    "    HumanMessage(content=\"Write Python code to find the make/model combinations of electric car in Washington state.\")\n",
    "]\n",
    "\n",
    "# Pass your message to GPT. Assign to rsps_python_top_models.\n",
    "rsps_python_top_models = chat(msgs_python_top_models)\n",
    "\n",
    "# Display the response's Markdown content\n",
    "display(Markdown(rsps_python_top_models.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275758d7-5f0d-4b75-a61f-24c588a84bcb",
   "metadata": {},
   "source": [
    "## Task 5: Execute the Code Provided by GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27c93f-255c-4b90-b2e8-f38b7a5dcad9",
   "metadata": {},
   "source": [
    "You just asked GPT to write some code for you. Next you need to see if it worked, and fix it if it didn't. \n",
    "\n",
    "This is a standard workflow for interacting with generative AI: the AI acts as a junior data analyst who writes the code, then you act as the boss who reviews the work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bf0d5-6c65-4aef-8a40-72c84dba1f1e",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Review the work of your AI assistant.\n",
    "\n",
    "- Copy and paste the code generated by GPT into the next code cell and run it.\n",
    "- _Look at the result. Do you think it is correct?_*\n",
    "- If the code threw an error or gave a wrong answer, use the Workspace AI Assistant (also powered by GPT!) to fix and explain the code.\n",
    "\n",
    "*During testing of this code-along project, GPT sometimes wrote incorrect code when using `.sum()`. Double-check those function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37ec0b93-8a67-4f02-9de0-0be59127f70e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1695346948895,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Paste the code generated by GPT and run it\n\n# GPT gave me a wrong answer! \n# It tried to filter the rows becaues it didn't understand the difference between a state and a county in this context\n# The .groupby().sum() code is also wrong.\n\n# # Filter the dataset for electric cars in Washington state\n# washington_cars = electric_cars[electric_cars['county'] == 'Washington']\n\n# # Group the dataset by make and model, and sum the number of cars for each combination\n# make_model_counts = washington_cars.groupby(['make', 'model']).sum('n_cars').reset_index()\n\n# # Sort the make/model combinations by the number of cars in descending order\n# sorted_make_model_counts = make_model_counts.sort_values('n_cars', ascending=False)\n\n# # Get the top 5 most popular make/model combinations\n# top_5_make_model = sorted_make_model_counts.head(5)\n\n# # Print the results\n# print(top_5_make_model)\n",
    "outputsMetadata": {
     "0": {
      "height": 154,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Paste the code generated by GPT and run it\n",
    "# Filter the DataFrame to include only electric vehicles\n",
    "electric_vehicles = electric_cars[electric_cars['electric_vehicle_type'].str.contains('Electric Vehicle')]\n",
    "\n",
    "# Extract the unique make/model combinations\n",
    "make_model_combinations = electric_vehicles[['make', 'model']].drop_duplicates()\n",
    "\n",
    "# Print the make/model combinations\n",
    "print(make_model_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b52c8-324f-40bd-826d-2b10993267b2",
   "metadata": {},
   "source": [
    "## Task 6: Continue the Conversation to Create a Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f33e51-9614-4e11-8a27-ff735c184cec",
   "metadata": {},
   "source": [
    "Doing more analysis with GPT assistance is simply a case of continuing the conversation by appending new `HumanMessage` prompts to the message list, and calling the `chat()` function.\n",
    "\n",
    "The output from GPT is random, but when doing data analysis this isn't always desirable since you'd like your results to be reproducible. With large language models, the amount of randomness can be controled with a parameter known as \"temperature\".\n",
    "\n",
    "- `temperature` controls the randomness of the response. It ranges from `0` to `2` with zero meaning minimal randomness (to make it easier to reproduce results) and two meaning maximum randomness (often gives weird responses). If you use the OpenAI API directly, the default is `1`, but using LangChain reduces the default value to `0.7`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043595b5-1d29-4aac-a428-81703e1300d1",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Create a new OpenAI chat object with temperature set to zero. Assign to `chat0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "649a18e2-cc07-47fb-ab80-735df0366743",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10,
    "lastExecutedAt": 1714645800326,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Create a new OpenAI chat object with temperature set to zero. Assign to chat0.\nchat0 = ChatOpenAI(temperature=0)"
   },
   "outputs": [],
   "source": [
    "# Create a new OpenAI chat object with temperature set to zero. Assign to chat0.\n",
    "chat0 = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17e508-a08e-4b5e-bc6f-288065c395f9",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Work through the previous conversation flow again, appending the previous response and a new request for Python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type.\n",
    "    - The solution asks for Plotly Express code, but you can pick any Python data viz package you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed9bf9e6-9fac-4703-bcdb-4a327ba62724",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11322,
    "lastExecutedAt": 1714645817448,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Ask GPT for code for a bar plot, as detailed in the instructions\nmsgs_python_plot = msgs_python_top_models + [\n    rsps_python_top_models,\n    HumanMessage(content=\"Write some python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type. Use the plotly express package.\")\n]\n\nrsps_python_plot = chat0(msgs_python_plot)\n\ndisplay(Markdown(rsps_python_plot.content))"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type using the Plotly Express package, you can use the following Python code:\n",
       "\n",
       "```python\n",
       "import plotly.express as px\n",
       "\n",
       "# Filter the DataFrame to include only electric vehicles\n",
       "electric_vehicles = electric_cars[electric_cars['electric_vehicle_type'].str.contains('Electric Vehicle')]\n",
       "\n",
       "# Group the data by model year and electric vehicle type, and calculate the total count of electric cars\n",
       "grouped_data = electric_vehicles.groupby(['model_year', 'electric_vehicle_type'])['n_cars'].sum().reset_index()\n",
       "\n",
       "# Draw the bar plot using Plotly Express\n",
       "fig = px.bar(grouped_data, x='model_year', y='n_cars', color='electric_vehicle_type',\n",
       "             labels={'n_cars': 'Total Count of Electric Cars', 'model_year': 'Model Year'},\n",
       "             title='Total Count of Electric Cars by Model Year',\n",
       "             barmode='group')\n",
       "\n",
       "fig.show()\n",
       "```\n",
       "\n",
       "In this code snippet:\n",
       "- We first filter the `electric_cars` DataFrame to include only electric vehicles.\n",
       "- Then, we group the data by 'model_year' and 'electric_vehicle_type' columns and calculate the total count of electric cars for each combination.\n",
       "- Finally, we use Plotly Express to create a bar plot where the x-axis represents the model year, the y-axis represents the total count of electric cars, and the bars are colored by electric vehicle type.\n",
       "\n",
       "You can run this code in your Python environment to visualize the total count of electric cars by model year, with bars colored by electric vehicle type using Plotly Express."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ask GPT for code for a bar plot, as detailed in the instructions\n",
    "msgs_python_plot = msgs_python_top_models + [\n",
    "    rsps_python_top_models,\n",
    "    HumanMessage(content=\"Write some python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type. Use the plotly express package.\")\n",
    "]\n",
    "\n",
    "rsps_python_plot = chat0(msgs_python_plot)\n",
    "\n",
    "display(Markdown(rsps_python_plot.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cde113-6e53-4681-bec6-cfdc8d7d2de0",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "To see how much variation there is with temperature set to zero, ask GPT for the same thing again.\n",
    "\n",
    "- Call GPT again with the same message list and display the response.\n",
    "- _Look at the response content. How close is it to the previous response content?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a039f5eb-75a6-4cec-9d5f-a8bfa2afbff8",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 10781,
    "lastExecutedAt": 1714646471620,
    "lastExecutedByKernel": "16a0e095-363f-4dd4-ad80-f32c4fc13843",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Call GPT again with the same message list and display the response\nmsgs_python_plot = msgs_python_top_models + [\n    rsps_python_top_models,\n    HumanMessage(content=\"Write some python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type. Use the plotly express package.\")\n]\n\nrsps_python_plot = chat0(msgs_python_plot)\n\ndisplay(Markdown(rsps_python_plot.content))"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type using the Plotly Express package, you can use the following Python code:\n",
       "\n",
       "```python\n",
       "import plotly.express as px\n",
       "\n",
       "# Filter the DataFrame to include only electric vehicles\n",
       "electric_vehicles = electric_cars[electric_cars['electric_vehicle_type'].str.contains('Electric Vehicle')]\n",
       "\n",
       "# Group the data by model year and electric vehicle type, and sum the counts\n",
       "grouped_data = electric_vehicles.groupby(['model_year', 'electric_vehicle_type'])['n_cars'].sum().reset_index()\n",
       "\n",
       "# Draw the bar plot\n",
       "fig = px.bar(grouped_data, x='model_year', y='n_cars', color='electric_vehicle_type',\n",
       "             labels={'n_cars': 'Total Count of Electric Cars', 'model_year': 'Model Year'},\n",
       "             title='Total Count of Electric Cars by Model Year',\n",
       "             barmode='group')\n",
       "\n",
       "fig.show()\n",
       "```\n",
       "\n",
       "In this code snippet:\n",
       "- We first filter the `electric_cars` DataFrame to include only electric vehicles.\n",
       "- Then, we group the data by 'model_year' and 'electric_vehicle_type', and sum the counts of electric cars.\n",
       "- Finally, we use Plotly Express to create a bar plot where the x-axis represents the model year, the y-axis represents the total count of electric cars, and the bars are colored by electric vehicle type.\n",
       "\n",
       "You can run this code in your Python environment to visualize the total count of electric cars by model year, with bars colored by electric vehicle type using Plotly Express."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call GPT again with the same message list and display the response\n",
    "msgs_python_plot = msgs_python_top_models + [\n",
    "    rsps_python_top_models,\n",
    "    HumanMessage(content=\"Write some python code to draw a bar plot of the total count of electric cars by model year, with bars colored by electric vehicle type. Use the plotly express package.\")\n",
    "]\n",
    "\n",
    "rsps_python_plot = chat0(msgs_python_plot)\n",
    "\n",
    "display(Markdown(rsps_python_plot.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa57fd-5901-421c-bfd0-04cdb3bbfea4",
   "metadata": {},
   "source": [
    "## Task 7: Execute the Code Provided by GPT to See Your Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc700e-2633-4c1b-8fa6-635499c480dd",
   "metadata": {},
   "source": [
    "Setting temperature to zero removed all randomness so you got the same output twice. That makes your workflow more reproducible.\n",
    "\n",
    "The final task is to see the plot. As before, remember that GPT is only your assistant and you are the boss. Check the code and its output to make sure that you really have what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9521e-b190-4997-ab87-e55a4cd1d402",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Run the code and check that the plot is correct.\n",
    "\n",
    "- Run the bar plot code generated by GPT.\n",
    "- _Check that the output is suitable. If not, try changing your prompt in the previous task to improve the output (this is prompt engineering, which you'll see more of in the next code-along project in the series)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d09c630-472c-49d5-8986-0ce986439b96",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 467,
      "type": "plotly"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "displaylogo": false,
        "plotlyServerURL": "https://plot.ly",
        "toImageButtonOptions": {
         "filename": "DataLab plot",
         "format": "png",
         "height": 500,
         "scale": 2,
         "width": 700
        }
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "electric_vehicle_type=Battery Electric Vehicle (BEV)<br>Model Year=%{x}<br>Total Count of Electric Cars=%{y}<extra></extra>",
         "legendgroup": "Battery Electric Vehicle (BEV)",
         "marker": {
          "color": "#6568a0",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Battery Electric Vehicle (BEV)",
         "offsetgroup": "Battery Electric Vehicle (BEV)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          1997,
          1998,
          1999,
          2000,
          2002,
          2003,
          2008,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024
         ],
         "xaxis": "x",
         "y": [
          1,
          1,
          4,
          8,
          2,
          1,
          18,
          21,
          718,
          776,
          2926,
          1796,
          3625,
          3867,
          4466,
          10046,
          8771,
          9617,
          14998,
          23511,
          31360,
          271
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "hovertemplate": "electric_vehicle_type=Plug-in Hybrid Electric Vehicle (PHEV)<br>Model Year=%{x}<br>Total Count of Electric Cars=%{y}<extra></extra>",
         "legendgroup": "Plug-in Hybrid Electric Vehicle (PHEV)",
         "marker": {
          "color": "#43d7a4",
          "pattern": {
           "shape": ""
          }
         },
         "name": "Plug-in Hybrid Electric Vehicle (PHEV)",
         "offsetgroup": "Plug-in Hybrid Electric Vehicle (PHEV)",
         "orientation": "v",
         "showlegend": true,
         "textposition": "auto",
         "type": "bar",
         "x": [
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024
         ],
         "xaxis": "x",
         "y": [
          3,
          78,
          857,
          1639,
          1817,
          1309,
          1783,
          4108,
          4395,
          1945,
          1677,
          3686,
          4288,
          5719,
          371
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "legend": {
         "title": {
          "text": "electric_vehicle_type"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#6568a0",
           "#43d7a4",
           "#4095db",
           "#facc5f",
           "#cae279",
           "#7db64f",
           "#f08083",
           "#5bcdf2",
           "#f099dc",
           "#965858"
          ],
          "font": {
           "color": "#05192D",
           "family": "Studio-Feixen-Sans, Arial, sans-serif"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left",
           "font": {
            "family": "Studio-Feixen-Sans, Arial, sans-serif"
           }
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Total Count of Electric Cars by Model Year"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model Year"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Total Count of Electric Cars"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paste the code generated by GPT and run it\n",
    "import plotly.express as px\n",
    "\n",
    "# Filter the DataFrame to include only electric vehicles\n",
    "electric_vehicles = electric_cars[electric_cars['electric_vehicle_type'].str.contains('Electric Vehicle')]\n",
    "\n",
    "# Group the data by model year and electric vehicle type, and sum the counts\n",
    "grouped_data = electric_vehicles.groupby(['model_year', 'electric_vehicle_type'])['n_cars'].sum().reset_index()\n",
    "\n",
    "# Draw the bar plot\n",
    "fig = px.bar(grouped_data, x='model_year', y='n_cars', color='electric_vehicle_type',\n",
    "             labels={'n_cars': 'Total Count of Electric Cars', 'model_year': 'Model Year'},\n",
    "             title='Total Count of Electric Cars by Model Year',\n",
    "             barmode='group')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61448d-2571-4ffa-ac93-47d0eba4c94e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now seen how to access GPT through the OpenAI API both directly and using LangChain.\n",
    "\n",
    "You saw how GPT can be used to come up with ideas for analyses to perform and to write code for you.\n",
    "\n",
    "You also saw how to have an extended conversation and how to control the reproducibility of the responses."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
